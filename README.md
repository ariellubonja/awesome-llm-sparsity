# awesome-llm-sparsity

Collection of papers, algorithms, and other resources that show how sparsity emerges in LLMs, and how it can be utilized to optimize them


## Emergent Sparsity

| Paper | Code | Date |
|:----|  :----: | :---:|
|[The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers](https://arxiv.org/pdf/2210.06313.pdf) @ Google | ∅ | June-23
|[Learn To be Efficient: Build Structured Sparsity in Large Language Models](https://arxiv.org/pdf/2402.06126.pdf) @ UIUC, CMU | ∅ | Feb-24
|[ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](https://arxiv.org/pdf/2310.04564.pdf) @ Apple | ∅ | Oct-23
<br/>


## Approximation Methods

| Title & Authors | Selling Point | Links |
|:----|  :----: | :---:|
|[![Star](https://img.shields.io/github/stars/robertcsordas/moe.svg?style=social&label=Star)](https://github.com/robertcsordas/moe)<br>[Approximating Two-Layer Feedforward Networks for Efficient Transformers](https://arxiv.org/pdf/2310.10837.pdf) | Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to ap- proximate two-layer NNs (e.g., feedforward blocks of Transformers), including product- key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs.  | [Github](https://github.com/robertcsordas/moe) <br> [Paper](https://arxiv.org/pdf/2310.10837.pdf)|
<br/>


## Related Repos

[Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM)

<br/><br/><br/>

Thanks to [horseee](https://github.com/horseee/Awesome-Efficient-LLM/blob/main/README.md?plain=1) for the table!

